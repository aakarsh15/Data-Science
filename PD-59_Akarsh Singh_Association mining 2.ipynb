{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Name- Akarsh Singh\n",
        "\n",
        "##Roll No.- 59\n",
        "\n",
        "##PRN- 1032212466\n",
        "\n",
        "##Panel- D                        Batch- D2\n",
        "\n",
        "##Subject- DEC\n",
        "\n",
        "##Assignment No.- 6\n",
        "\n",
        "#Association Mining"
      ],
      "metadata": {
        "id": "jySN-M-nNPbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eIzt31Z9MzKh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Association rules using mlxtend Library\n",
        "### On a user defined dataset"
      ],
      "metadata": {
        "id": "T3c_QXRCNVuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "import pandas as pd\n",
        "\n",
        "# Sample dataset\n",
        "data = {'TransactionID': [1, 2, 3, 4, 5],\n",
        "        'Items': [['A', 'B', 'D'],\n",
        "                  ['B', 'C', 'E'],\n",
        "                  ['A', 'B', 'C', 'E'],\n",
        "                  ['B', 'E'],\n",
        "                  ['A', 'C', 'D']]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Convert the 'Items' column to a one-hot encoded format\n",
        "ohe_df = pd.get_dummies(df['Items'].apply(pd.Series).stack()).sum(level=0)\n",
        "\n",
        "# Apply Apriori algorithm to find frequent itemsets\n",
        "frequent_itemsets = apriori(ohe_df, min_support=0.4, use_colnames=True)\n",
        "\n",
        "# Generate association rules\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "\n",
        "# Display the frequent itemsets\n",
        "print(\"Frequent Itemsets:\")\n",
        "print(frequent_itemsets)\n",
        "\n",
        "# Display the association rules\n",
        "print(\"\\nAssociation Rules:\")\n",
        "print(rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w3_pD63NZry",
        "outputId": "0553b1e6-9068-4281-940d-e77b9b3eb475"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets:\n",
            "    support   itemsets\n",
            "0       0.6        (A)\n",
            "1       0.8        (B)\n",
            "2       0.6        (C)\n",
            "3       0.4        (D)\n",
            "4       0.6        (E)\n",
            "5       0.4     (B, A)\n",
            "6       0.4     (C, A)\n",
            "7       0.4     (D, A)\n",
            "8       0.4     (B, C)\n",
            "9       0.6     (E, B)\n",
            "10      0.4     (E, C)\n",
            "11      0.4  (E, B, C)\n",
            "\n",
            "Association Rules:\n",
            "  antecedents consequents  antecedent support  consequent support  support  \\\n",
            "0         (D)         (A)                 0.4                 0.6      0.4   \n",
            "1         (E)         (B)                 0.6                 0.8      0.6   \n",
            "2         (B)         (E)                 0.8                 0.6      0.6   \n",
            "3      (E, C)         (B)                 0.4                 0.8      0.4   \n",
            "4      (B, C)         (E)                 0.4                 0.6      0.4   \n",
            "\n",
            "   confidence      lift  leverage  conviction  zhangs_metric  \n",
            "0        1.00  1.666667      0.16         inf       0.666667  \n",
            "1        1.00  1.250000      0.12         inf       0.500000  \n",
            "2        0.75  1.250000      0.12         1.6       1.000000  \n",
            "3        1.00  1.250000      0.08         inf       0.333333  \n",
            "4        1.00  1.666667      0.16         inf       0.666667  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<ipython-input-2-bafd78b70df0>:16: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
            "  ohe_df = pd.get_dummies(df['Items'].apply(pd.Series).stack()).sum(level=0)\n",
            "/usr/local/lib/python3.10/dist-packages/mlxtend/frequent_patterns/fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "dYRo2zScNfes",
        "outputId": "d4483edc-dce4-4c11-e79c-f926fc16ccd0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-31e21ffc-0f65-4587-bb2a-ee73f4d2064c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-31e21ffc-0f65-4587-bb2a-ee73f4d2064c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving store_data.csv to store_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file\n",
        "df = pd.read_csv(\"store_data.csv\", header=None)\n",
        "\n",
        "# Create a new DataFrame with 'Transaction ID' and 'Items' columns\n",
        "result_df = pd.DataFrame(columns=['Transaction ID', 'Items'])\n",
        "\n",
        "# Iterate through each row and append it to the result DataFrame\n",
        "for idx, row in df.iterrows():\n",
        "    transaction_id = idx\n",
        "    items = row.dropna().tolist()\n",
        "    result_df = pd.concat([result_df, pd.DataFrame({'Transaction ID': [transaction_id], 'Items': [items]})], ignore_index=True)\n",
        "\n",
        "# Convert the lists in the \"Items\" column to strings\n",
        "result_df['Items'] = result_df['Items'].apply(lambda x: ', '.join(map(str, x)))\n",
        "\n",
        "# Set 'Transaction ID' as the index\n",
        "result_df.set_index('Transaction ID', inplace=True)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(result_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWxzWBGnNyTl",
        "outputId": "091fe320-b9b7-4ce8-918d-88ab8fc2e523"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                            Items\n",
            "Transaction ID                                                   \n",
            "0               shrimp, almonds, avocado, vegetables mix, gree...\n",
            "1                                        burgers, meatballs, eggs\n",
            "2                                                         chutney\n",
            "3                                                 turkey, avocado\n",
            "4               mineral water, milk, energy bar, whole wheat r...\n",
            "...                                                           ...\n",
            "7496                              butter, light mayo, fresh bread\n",
            "7497            burgers, frozen vegetables, eggs, french fries...\n",
            "7498                                                      chicken\n",
            "7499                                          escalope, green tea\n",
            "7500            eggs, frozen smoothie, yogurt cake, low fat yo...\n",
            "\n",
            "[7501 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Association rules using mlxtend library\n"
      ],
      "metadata": {
        "id": "JfwN5cikN5Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Convert the lists in the \"Items\" column to strings\n",
        "result_df['Items'] = result_df['Items'].apply(lambda x: ', '.join(map(str, x)))\n",
        "\n",
        "# One-hot encode the Items column\n",
        "ohe_df = result_df['Items'].str.get_dummies(', ')\n",
        "\n",
        "# Apply Apriori algorithm to find frequent itemsets\n",
        "frequent_itemsets = apriori(ohe_df, min_support=0.4, use_colnames=True)\n",
        "\n",
        "# Remove empty string and space from itemsets\n",
        "frequent_itemsets = frequent_itemsets[\n",
        "    frequent_itemsets['itemsets'].apply(\n",
        "        lambda x: len(x) > 0 and all(item != '' and item != ' ' and item != ',' for item in x)\n",
        "    )\n",
        "]\n",
        "\n",
        "# Generate association rules\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "\n",
        "# Display the frequent itemsets (excluding empty string, space, and comma)\n",
        "print(\"Frequent Itemsets:\")\n",
        "print(frequent_itemsets)\n",
        "\n",
        "# Display the association rules\n",
        "print(\"\\nAssociation Rules:\")\n",
        "print(rules)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGf8gXrvN7e6",
        "outputId": "2193bf6c-c649-459b-866d-9243c9bce811"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/mlxtend/frequent_patterns/fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets:\n",
            "        support                  itemsets\n",
            "2      0.843221                       (a)\n",
            "3      0.457672                       (b)\n",
            "4      0.729636                       (c)\n",
            "5      0.952673                       (e)\n",
            "6      0.478336                       (f)\n",
            "...         ...                       ...\n",
            "10501  0.403280  (n, a, e, o, t, l, s, i)\n",
            "10502  0.427143  (n, a, e, t, l, s, i, r)\n",
            "10503  0.413678  (a, e, o, t, l, s, i, r)\n",
            "10504  0.430476  (n, a, e, o, t, s, i, r)\n",
            "10505  0.418611  (n, a, e, o, t, l, s, r)\n",
            "\n",
            "[2768 rows x 2 columns]\n",
            "\n",
            "Association Rules:\n",
            "      antecedents         consequents  antecedent support  consequent support  \\\n",
            "0             (b)                 (a)            0.457672            0.843221   \n",
            "1             (c)                 (a)            0.729636            0.843221   \n",
            "2             (a)                 (c)            0.843221            0.729636   \n",
            "3             (e)                 (a)            0.952673            0.843221   \n",
            "4             (a)                 (e)            0.843221            0.952673   \n",
            "...           ...                 ...                 ...                 ...   \n",
            "70832      (n, o)  (a, e, t, l, s, r)            0.571124            0.490068   \n",
            "70833      (l, n)  (a, e, o, t, s, r)            0.555526            0.498867   \n",
            "70834      (l, o)  (n, a, e, t, s, r)            0.592588            0.518331   \n",
            "70835      (l, s)  (n, a, e, o, t, r)            0.579656            0.492868   \n",
            "70836      (l, r)  (n, a, e, o, t, s)            0.582856            0.477403   \n",
            "\n",
            "        support  confidence      lift  leverage  conviction  zhangs_metric  \n",
            "0      0.430343    0.940285  1.115112  0.044424    2.625476       0.190344  \n",
            "1      0.651646    0.893112  1.059167  0.036402    1.466756       0.206617  \n",
            "2      0.651646    0.772806  1.059167  0.036402    1.190015       0.356308  \n",
            "3      0.815625    0.856143  1.015325  0.012311    1.089828       0.318924  \n",
            "4      0.815625    0.967273  1.015325  0.012311    1.446103       0.096274  \n",
            "...         ...         ...       ...       ...         ...            ...  \n",
            "70832  0.418611    0.732960  1.495629  0.138721    1.909571       0.772682  \n",
            "70833  0.418611    0.753540  1.510503  0.141477    2.033322       0.760379  \n",
            "70834  0.418611    0.706412  1.362859  0.111454    1.640628       0.653510  \n",
            "70835  0.418611    0.722171  1.465244  0.132917    1.825341       0.755380  \n",
            "70836  0.418611    0.718207  1.504404  0.140354    1.854541       0.803762  \n",
            "\n",
            "[70837 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Association Rules Without mlxtend library\n",
        "### On a small dataset"
      ],
      "metadata": {
        "id": "Q9hZHixMOBT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import chain, combinations\n",
        "\n",
        "def powerset(iterable):\n",
        "    s = list(iterable)\n",
        "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
        "\n",
        "def generate_frequent_itemsets(data, min_support):\n",
        "    transactions = data['Items'].tolist()\n",
        "    item_counts = {}\n",
        "\n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            item_counts[item] = item_counts.get(item, 0) + 1\n",
        "\n",
        "    # Filter items based on minimum support\n",
        "    frequent_items = {item for item, count in item_counts.items() if count / len(transactions) >= min_support}\n",
        "\n",
        "    # Generate frequent itemsets\n",
        "    frequent_itemsets = []\n",
        "    for itemset in powerset(frequent_items):\n",
        "        if len(itemset) > 0:\n",
        "            frequent_itemsets.append(set(itemset))\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "def generate_association_rules(frequent_itemsets, data, min_confidence):\n",
        "    rules = []\n",
        "    transactions = data['Items'].tolist()\n",
        "\n",
        "    for itemset in frequent_itemsets:\n",
        "        for item in itemset:\n",
        "            antecedent = itemset - {item}\n",
        "            support_itemset = sum(1 for transaction in transactions if itemset.issubset(transaction)) / len(transactions)\n",
        "            support_antecedent = sum(1 for transaction in transactions if antecedent.issubset(transaction)) / len(transactions)\n",
        "            confidence = support_itemset / support_antecedent if support_antecedent > 0 else 0\n",
        "\n",
        "            if confidence >= min_confidence:\n",
        "                rules.append({\n",
        "                    'antecedents': antecedent,\n",
        "                    'consequents': {item},\n",
        "                    'antecedent support': support_antecedent,\n",
        "                    'consequent support': support_itemset,\n",
        "                    'support': support_itemset,\n",
        "                    'confidence': confidence,\n",
        "                    'lift': confidence / support_itemset,\n",
        "                    'leverage': support_itemset - (support_antecedent * support_itemset),\n",
        "                    'conviction': (1 - support_antecedent) / (1 - confidence) if confidence < 1 else float('inf')\n",
        "                })\n",
        "\n",
        "    return rules\n",
        "\n",
        "# Sample data\n",
        "data = pd.DataFrame({'Items': [['a', 'b', 'c'], ['a', 'b'], ['a', 'c'], ['b', 'c'], ['a'], ['b']]})\n",
        "\n",
        "# Set minimum support and confidence thresholds\n",
        "min_support = 0.4\n",
        "min_confidence = 0.1\n",
        "\n",
        "# Generate frequent itemsets\n",
        "frequent_itemsets = generate_frequent_itemsets(data, min_support)\n",
        "\n",
        "# Generate association rules\n",
        "rules = generate_association_rules(frequent_itemsets, data, min_confidence)\n",
        "\n",
        "# Convert rules to DataFrame for better formatting\n",
        "rules_df = pd.DataFrame(rules)\n",
        "\n",
        "# Display the association rules DataFrame\n",
        "print(\"Association Rules:\")\n",
        "print(rules_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLGPGoygODdf",
        "outputId": "e481e2e6-1595-4221-a084-85785ddf5722"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Association Rules:\n",
            "   antecedents consequents  antecedent support  consequent support   support  \\\n",
            "0           {}         {c}            1.000000            0.500000  0.500000   \n",
            "1           {}         {b}            1.000000            0.666667  0.666667   \n",
            "2           {}         {a}            1.000000            0.666667  0.666667   \n",
            "3          {b}         {c}            0.666667            0.333333  0.333333   \n",
            "4          {c}         {b}            0.500000            0.333333  0.333333   \n",
            "5          {a}         {c}            0.666667            0.333333  0.333333   \n",
            "6          {c}         {a}            0.500000            0.333333  0.333333   \n",
            "7          {a}         {b}            0.666667            0.333333  0.333333   \n",
            "8          {b}         {a}            0.666667            0.333333  0.333333   \n",
            "9       {b, a}         {c}            0.333333            0.166667  0.166667   \n",
            "10      {c, a}         {b}            0.333333            0.166667  0.166667   \n",
            "11      {c, b}         {a}            0.333333            0.166667  0.166667   \n",
            "\n",
            "    confidence  lift  leverage  conviction  \n",
            "0     0.500000   1.0  0.000000    0.000000  \n",
            "1     0.666667   1.0  0.000000    0.000000  \n",
            "2     0.666667   1.0  0.000000    0.000000  \n",
            "3     0.500000   1.5  0.111111    0.666667  \n",
            "4     0.666667   2.0  0.166667    1.500000  \n",
            "5     0.500000   1.5  0.111111    0.666667  \n",
            "6     0.666667   2.0  0.166667    1.500000  \n",
            "7     0.500000   1.5  0.111111    0.666667  \n",
            "8     0.500000   1.5  0.111111    0.666667  \n",
            "9     0.500000   3.0  0.111111    1.333333  \n",
            "10    0.500000   3.0  0.111111    1.333333  \n",
            "11    0.500000   3.0  0.111111    1.333333  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Association Rules without mlxtend Library\n",
        "## on a big CSV file\n",
        "\n",
        "#### file too big to iterate manually"
      ],
      "metadata": {
        "id": "UzgC5ju3OIVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import chain, combinations\n",
        "\n",
        "def powerset(iterable):\n",
        "    s = list(iterable)\n",
        "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
        "\n",
        "def generate_frequent_itemsets(data, min_support):\n",
        "    transactions = data['Items'].tolist()\n",
        "    item_counts = {}\n",
        "\n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            item_counts[item] = item_counts.get(item, 0) + 1\n",
        "\n",
        "    # Filter items based on minimum support\n",
        "    frequent_items = {item for item, count in item_counts.items() if count / len(transactions) >= min_support}\n",
        "\n",
        "    # Generate frequent itemsets\n",
        "    frequent_itemsets = []\n",
        "    for itemset in powerset(frequent_items):\n",
        "        if len(itemset) > 0:\n",
        "            frequent_itemsets.append(set(itemset))\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "def generate_association_rules(frequent_itemsets, data, min_confidence):\n",
        "    rules = []\n",
        "    transactions = data['Items'].tolist()\n",
        "\n",
        "    for itemset in frequent_itemsets:\n",
        "        for item in itemset:\n",
        "            antecedent = itemset - {item}\n",
        "            support_itemset = sum(1 for transaction in transactions if itemset.issubset(transaction)) / len(transactions)\n",
        "            support_antecedent = sum(1 for transaction in transactions if antecedent.issubset(transaction)) / len(transactions)\n",
        "            confidence = support_itemset / support_antecedent if support_antecedent > 0 else 0\n",
        "\n",
        "            if confidence >= min_confidence:\n",
        "                rules.append({\n",
        "                    'antecedents': antecedent,\n",
        "                    'consequents': {item},\n",
        "                    'antece dent support': support_antecedent,\n",
        "                    'consequent support': support_itemset,\n",
        "                    'support': support_itemset,\n",
        "                    'confidence': confidence,\n",
        "                    'lift': confidence / support_itemset,\n",
        "                    'leverage': support_itemset - (support_antecedent * support_itemset),\n",
        "                    'conviction': (1 - support_antecedent) / (1 - confidence) if confidence < 1 else float('inf')\n",
        "                })\n",
        "\n",
        "    return rules\n",
        "\n",
        "# Sample data\n",
        "data = result_df\n",
        "\n",
        "# Set minimum support and confidence thresholds\n",
        "min_support = 0.4\n",
        "min_confidence = 0.1\n",
        "\n",
        "# Generate frequent itemsets\n",
        "frequent_itemsets = generate_frequent_itemsets(data, min_support)\n",
        "\n",
        "# Generate association rules\n",
        "rules = generate_association_rules(frequent_itemsets, data, min_confidence)\n",
        "\n",
        "# Convert rules to DataFrame for better formatting\n",
        "rules_df = pd.DataFrame(rules)\n",
        "\n",
        "# Display the association rules DataFrame\n",
        "print(\"Association Rules:\")\n",
        "print(rules_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuCySI_SOKQ6",
        "outputId": "7fb80a7c-171c-438e-fbc2-bdb7cabbb3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    }
  ]
}