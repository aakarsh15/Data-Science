{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name- Akarsh Singh\n",
        "\n",
        "Roll- 59\n",
        "\n",
        "PRN- 1032212466\n",
        "\n",
        "Panel- D      Batch- D2\n",
        "\n",
        "Subject- DEC\n",
        "\n",
        "LAB ASSIGNMENT NO.- 3"
      ],
      "metadata": {
        "id": "GUwzNPm02moi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aim: Preprocess data Using Python\n",
        "\n",
        "Problem Statement: Data Pre-Processing using Python (Part II)\n",
        "\n",
        "***OBJECTIVES:***\n",
        "\n",
        "***1.  Data integration:***\n",
        "\n",
        "Data integration is the process of combining data from multiple sources into a single, unified view. This can be a complex task, as the data may be in different formats, have different structures, and contain different levels of quality. Common data integration challenges include:\n",
        "\n",
        "\n",
        "*  Data heterogeneity: Data from different sources may have different formats, structures, and semantics.\n",
        "*   Data redundancy: Data may be duplicated in multiple sources.\n",
        "*   Data inconsistency: Data may be inconsistent between different sources.\n",
        "\n",
        "\n",
        "\n",
        "1.   **Data Redundancy and Correlation Analysis:**\n",
        "Data redundancy and correlation analysis is the process of identifying and removing duplicate and redundant data from a dataset. This can be done by comparing the values of different attributes and identifying pairs of attributes that are highly correlate\n",
        "2.  **Tuple Duplication:**\n",
        " Tuple duplication is the presence of duplicate data records in a dataset. This can occur due to errors in data entry or replication, or because the same data is collected from multiple sources.\n",
        "\n",
        "***2. Data Transformation:***\n",
        "\n",
        " Data transformation is the process of converting data from one format to another. This can be necessary to prepare data for analysis, storage, or integration. Common data transformation tasks include:\n",
        "\n",
        " *  *Data cleansing:* This involves removing errors and inconsistencies from the data.\n",
        " *   *Data formatting:* This involves converting the data to a specific format, such as CSV, JSON, or XML.\n",
        "\n",
        " * *Data aggregation:* This involves combining multiple data points into a single summary value.\n",
        " *   *Feature engineering:* This involves creating new features from existing features to improve the performance of machine learning models.\n",
        "\n",
        "  1.  **Normalization - Min Max, Z-score:**\n",
        "Normalization is a data transformation technique that scales the values of all attributes to a common range. This can be useful for machine learning algorithms, as it can improve the performance of the algorithm and reduce the risk of overfitting.\n",
        "\n",
        "   *   Min-max normalization: This scales the values of all attributes to the range [0, 1].\n",
        "   *   Z-score normalization: This scales the values of all attributes to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "\n",
        "  2.  **Data Smoothening-Binning Methods (on data set such as CSV/xls file):**\n",
        "  Data smoothing is a data transformation technique that reduces noise and variability in the data. This can be useful for identifying trends and patterns in the data.\n",
        "\n",
        "\n",
        "  *   *Binning:* This involves dividing the range of values for an attribute into a number of bins. The values within each bin are then replaced by the average value of the bin.\n",
        "\n",
        "***3.  Outlier Analysis:***\n",
        "\n",
        "Outlier analysis is the process of identifying data points that are significantly different from the rest of the data. Outliers can be caused by errors in data collection or entry, or they may represent genuine anomalies in the data.\n",
        "\n",
        "***4. Data Reduction:***\n",
        "\n",
        "Data reduction is the process of reducing the size of a dataset while preserving the most important information. This can be useful for improving the performance of machine learning algorithms or for reducing the storage requirements of a dataset.\n",
        "\n",
        "*   *PCA Method:* Principal component analysis (PCA) is a data reduction technique that is used to identify the most important features in a dataset. PCA works by transforming the data into a new set of features, called principal components. The principal components are ordered by their importance, with the first principal component being the most important and the last principal component being the least important.\n",
        "PCA can be used to reduce the dimensionality of a dataset by selecting a subset of the principal components. This subset of principal components can then be used for analysis or machine learning without sacrificing too much information.\n",
        "I hope this helps to define these terms clearly. Please let me know if you have any other questions.\n",
        "\n",
        "\n",
        "***THEORY:***\n",
        "\n",
        "\n",
        "\n",
        "*   *Pearson Correlation: NumPy and SciPy Implementation:*\n",
        "NumPy and SciPy are two popular Python libraries for scientific computing. Both libraries provide functions for calculating the Pearson correlation coefficient.                                                                            \n",
        "The NumPy function for calculating the Pearson correlation coefficient is np.corrcoef(). It takes two arrays as input and returns a correlation matrix. The correlation matrix is a square matrix, where each element on the diagonal is 1.0 and each element off the diagonal is the Pearson correlation coefficient between the two corresponding arrays.                                             \n",
        "The SciPy function for calculating the Pearson correlation coefficient is scipy.stats.pearsonr(). It also takes two arrays as input, but it returns a tuple containing the Pearson correlation coefficient and the p-value. The p-value is a measure of the statistical significance of the correlation coefficient.\n",
        "\n",
        "\n",
        "*   *Pearson Correlation: Pandas Implementation:*\n",
        "Pandas is a Python library for data analysis and manipulation. Pandas provides a DataFrame object, which is a two-dimensional data structure that is similar to a spreadsheet.                                   \n",
        "To calculate the Pearson correlation coefficient between two columns in a Pandas DataFrame, you can use the corr() method. The corr() method returns a correlation matrix, where each element on the diagonal is 1.0 and each element off the diagonal is the Pearson correlation coefficient between the two corresponding columns.\n",
        "\n",
        "\n",
        "\n",
        "*   *Visualization of Correlation:*\n",
        "Correlation matrices can be visualized using heatmaps. A heatmap is a color-coded matrix, where the color of each element represents the value of the element. Heatmaps can be used to quickly identify patterns and trends in correlation matrices.                       \n",
        "To create a heatmap of a correlation matrix in Python, you can use the seaborn.heatmap() function. The seaborn.heatmap() function takes a correlation matrix as input and returns a heatmap object.\n",
        "\n",
        "    *   *Heatmaps of Correlation Matrices:*\n",
        "Heatmaps of correlation matrices can be used to identify the following:\n",
        "     1.     *Positive and negative correlations:*\n",
        "Positive correlations are indicated by red and orange colors, while negative correlations are indicated by blue and green colors.\n",
        "     2.    *Strong and weak correlations:* The intensity of the color indicates the strength of the correlation. A bright color indicates a strong correlation, while a faint color indicates a weak correlation.\n",
        "\n",
        "     3.    *Clusters of correlated features:* Features that are highly correlated with each other will appear in clusters in the heatmap.\n",
        "\n",
        "*   *Feature Normalization and their techniques:*\n",
        "Feature normalization is the process of scaling data to a common range of values. This can be useful for improving the accuracy and efficiency of data analysis and machine learning.                       \n",
        "There are a variety of feature normalization techniques, including:\n",
        "\n",
        "      *   *Min-max normalization:* This technique scales data to a range of values from 0 to 1.\n",
        "      *  Z-score normalization:* This technique scales data to a mean of 0 and a standard deviation of 1.\n",
        "      *  *Standardization:* This technique scales data to a mean of 0 and a unit variance.                        \n",
        "\n",
        "  The choice of feature normalization technique depends on the specific data analysis or machine learning task."
      ],
      "metadata": {
        "id": "bywQhxv52uiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's perform a step-by-step Data Integration using Python."
      ],
      "metadata": {
        "id": "2R3QZFOh3EhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data redundancy has a number of advantages and disadvantages.**\n",
        "\n",
        "*Advantages:*\n",
        "\n",
        "Improved data availability and reliability: If one copy of the data is lost or corrupted, the other copies can still be used.\n",
        "Improved data integrity: Multiple copies of the data can be compared to detect and correct errors.\n",
        "Increased fault tolerance: The system can continue to function even if one copy of the data is lost or corrupted.\n",
        "\n",
        "\n",
        "*Disadvantages:*\n",
        "\n",
        "Increased storage requirements: Multiple copies of the data must be maintained.\n",
        "Increased complexity of the system: Managing multiple copies of the data can be difficult and time-consuming.\n",
        "Increased risk of data inconsistencies: If updates are not properly propagated to all copies of the data, the different copies may become out of sync."
      ],
      "metadata": {
        "id": "D7ka-_FZ3e9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Use files.upload() to upload a file\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ktY7aioH3rhh",
        "outputId": "fbeddeb1-e79c-438f-ef0b-8448d3f67a71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7efda99d-ee5d-4cd9-b0fa-dad6dbadfb3f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7efda99d-ee5d-4cd9-b0fa-dad6dbadfb3f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving PNQ_AQI.csv to PNQ_AQI.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TiIxEzez6lpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data redundancy and tuple duplication**\n",
        "\n",
        "Data redundancy occurs when the same data is stored in multiple locations. This can happen for a variety of reasons, such as:\n",
        "\n",
        "    1.  Data duplication during data entry or transfer\n",
        "    2.  Data synchronization errors\n",
        "    3.  Different systems storing the same data for different purposes"
      ],
      "metadata": {
        "id": "lV5FnB7L3-Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(\"PNQ_AQI.csv\")\n"
      ],
      "metadata": {
        "id": "u50MTpA34yaq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the drop_duplicates() method in Pandas:\n",
        "> The drop_duplicates() method in Pandas can be used to remove duplicate rows from a DataFrame. The drop_duplicates() method takes a number of parameters, including the subset parameter, which can be used to specify the columns to be used to identify duplicate rows.\n",
        "\n"
      ],
      "metadata": {
        "id": "iI3ISNDy48hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "# Remove tuple duplication\n",
        "df.drop_duplicates(subset=df.columns, inplace=True)\n",
        "# Save the results to a new CSV file\n",
        "df.to_csv(\"no_duplicates.csv\", index=False)"
      ],
      "metadata": {
        "id": "AAD7STGH5B3h"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1  = pd.read_csv(\"no_duplicates.csv\")\n",
        "print(df1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSeVkC3Y5yT2",
        "outputId": "4154408f-a714-4f5f-be6b-3af126e512db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Date SO2 µg/m3 Nox µg/m3  RSPM µg/m3    SPM  CO2 µg/m3  \\\n",
            "0     2009-01-01 00:00:00        15        53       179.0    NaN        NaN   \n",
            "1     2009-02-01 00:00:00        15        48       156.0    NaN        NaN   \n",
            "2     2009-03-01 00:00:00        13        51       164.0    NaN        NaN   \n",
            "3     2009-04-01 00:00:00         8        37       135.0    NaN        NaN   \n",
            "4     2009-07-01 00:00:00        13        36       140.0    NaN        NaN   \n",
            "...                   ...       ...       ...         ...    ...        ...   \n",
            "8411           18-01-2019        41       100       134.0  300.0        NaN   \n",
            "8412           19-01-2019        50       132       172.0  446.0        NaN   \n",
            "8413           23-01-2019        47       138       210.0  558.0        NaN   \n",
            "8414  2019-02-02 00:00:00        29        88        82.0  259.0        NaN   \n",
            "8415  2019-09-02 00:00:00        39        96       171.0  422.0        NaN   \n",
            "\n",
            "        AQI   Location  \n",
            "0     153.0    MPCB-KR  \n",
            "1     137.0    MPCB-KR  \n",
            "2     143.0    MPCB-KR  \n",
            "3     123.0    MPCB-KR  \n",
            "4     127.0    MPCB-KR  \n",
            "...     ...        ...  \n",
            "8411  123.0  MPCB-SWGT  \n",
            "8412  152.0  MPCB-SWGT  \n",
            "8413  173.0  MPCB-SWGT  \n",
            "8414  108.0  MPCB-SWGT  \n",
            "8415  147.0  MPCB-SWGT  \n",
            "\n",
            "[8416 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing duplicate rows and tuple duplication is an important task for ensuring the accuracy, efficiency, and consistency of data. By using the methods described above, organizations can improve the quality of their data systems."
      ],
      "metadata": {
        "id": "jAXNDdCh59pv"
      }
    }
  ]
}